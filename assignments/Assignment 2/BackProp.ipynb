{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c8bd72e320e83856a47b8b3e083718a",
     "grade": false,
     "grade_id": "cell-d355d14a5c1920b0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# BackPropagation from Scratch - An intuitive Approach \n",
    "## Teaching Assistants : Alex Mathai, Rishav Sinha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e184bbdbc57ae2b2821ff6c8bb8f3068",
     "grade": false,
     "grade_id": "cell-36dbed071c7a964f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Backpropagation from Scratch\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0af06b6f773dfd5c61e67dfd8fbbe94d",
     "grade": false,
     "grade_id": "cell-b59afc245b4bfab6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Most beginners try understanding back-propagation by rote learning formulae. This assignment is intended to convince you that such an approach is not required, as back propagation is quite inutitive if taught corretly. In  this assignment I aim to teach you how to think about losses and gradients and how to derive back propagation by yourself without having to rely on deep learning frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8bcb54a938de432b2135e4845d99368",
     "grade": false,
     "grade_id": "cell-0b202cffd4e5e2a8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Simple Beginnings with A Simple Network\n",
    "\n",
    "Consider the simple network below. There is a single neuron with two inputs $X_{2}$ and $X_{1}$ connected by two weights ($W_{2}$ and $W_{1}$).\n",
    "\n",
    "As shown below, let $W_{1} = 0.3$ and $W_{2}=0.7$.\n",
    "\n",
    "So the network below can be summarized as $\\hat{Y} = W_{2}*X_{2} + W_{1}*X{1}$\n",
    "\n",
    "Let the ground truth be $Y_{g}$, which along with $\\hat{Y}$ is fed to the squared error loss function.\n",
    "\n",
    "So the $Loss =  (1/2)*(Y_{g} - \\hat{Y})^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c12437fcc96e1846f59f98790df101bd",
     "grade": false,
     "grade_id": "cell-6936c3ac5d87cef6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Images/Simple_Network.jpg\" width=300 height=300>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"Images/Simple_Network.jpg\" width=300 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "576c413064a42fd6a492d844fc8f522e",
     "grade": false,
     "grade_id": "cell-d1874760301e51e6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Concept 1 - Anti-Gradients are good for loss functions\n",
    "\n",
    "To motivate this concept, let $X_{1} = -100$ and $X_{2}=1000$. Hence $\\hat{Y} = 670$. Now let us say, that the $Y_{g} = 1000$. \n",
    "\n",
    "It is apparent, that in order to reduce the loss, we need to increase $\\hat{Y}$. Let us study how our loss changes by varying the weights of the network for this input.\n",
    "\n",
    "Observations about Varying $W_{1}$\n",
    "1. By increasing $W_{1}$, we increase $\\hat{Y}$ thus decreasing $Loss$.\n",
    "2. As increasing $W_{1}$  decreases $Error$, we can say that $(\\frac{\\partial Loss}{\\partial W_{1}}) < 0$ and so $(-\\frac{\\partial Loss}{\\partial W_{1}}) > 0$.\n",
    "3. As our aim is to reduce loss, one subtask is to increase $W_{1}$\n",
    "\n",
    "Observations about Varying $W_{2}$\n",
    "1. By increasing $W_{2}$, we decrease $\\hat{Y}$ thus increasing $Loss$.\n",
    "2. As increasing $W_{2}$  increases $Loss$, we can say that $(\\frac{\\partial Loss}{\\partial W_{2}}) > 0$ and so $(-\\frac{\\partial Loss}{\\partial W_{2}}) < 0$.\n",
    "3. As our aim is to reduce loss, one subtask is to decrease $W_{2}$\n",
    "\n",
    "What do we want?\n",
    "1. From our observations above, we desire to increase $W_{1}$ and decrease $W_{2}$\n",
    "2. These desired changes can be summarized in the following formula : $W_{i} = W_{i} -\\alpha*\\frac{\\partial Loss}{\\partial W_{i}}$ or $W_{i} = W_{i} + \\alpha*(-\\frac{\\partial Loss}{\\partial W_{i}})$\n",
    "3. Verify for yourself, that by moving each $W_{i}$ in this manner we manage to reduce loss.\n",
    "\n",
    "Hence if we change any parameter ($W_{i}$) along the **opposite direction** of $\\frac{\\partial Loss}{\\partial W_{i}}$, we will end up reducing the loss. This is called moving along the anti-gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30d9353a576c317238b6ed1500c3089f",
     "grade": false,
     "grade_id": "cell-b9169380c8d42654",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_gradient_change(weight:np.ndarray,\n",
    "                          gradient:np.ndarray,\n",
    "                          alpha:float) -> np.ndarray:\n",
    "    \"\"\" Takes the weight and applies the gradient change as mentioned above. It then\n",
    "    returns the changed weight. \n",
    "    \n",
    "    Max Marks : 1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    new_weight = weight - alpha * gradient\n",
    "    return new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f08d4564ee46141bcc8ba1c0b55fa2ab",
     "grade": true,
     "grade_id": "cell-9058d595be63e99b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!\n",
    "\n",
    "assert np.array_equal(apply_gradient_change(np.array([1.,2.,3.,4.]),np.array([1.,2.,3.,4.]),0.001),np.array([0.999,1.998,2.997,3.996]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c65dd06ee341da05c294c66487ccec1",
     "grade": false,
     "grade_id": "cell-11fe84fd31c98966",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Adding more neurons & the Error Graph\n",
    "\n",
    "Let us add a few more neurons to the simple network in order to introduce another concept. Consider the network below. We will refer to this network as *Complex Network* in the following discussions.\n",
    "\n",
    "1. $ \\hat{Y} = W_{1}*Z_{1} + W_{2}*Z_{2}$ : Here Pred is the same as $\\hat{Y}$\n",
    "    * $\\frac{\\partial \\hat{Y}}{\\partial Z_{1}} = \\frac{\\partial (W_{1}*Z_{1} + W_{2}*Z_{2})}{\\partial Z_{1}} = W_{1}$\n",
    "    * $\\frac{\\partial \\hat{Y}}{\\partial Z_{2}} = \\frac{\\partial (W_{1}*Z_{1} + W_{2}*Z_{2})}{\\partial Z_{2}} = W_{2}$\n",
    "\n",
    "2. $Z_{1} = W_{3}*Z_{3} + W_{4}*Z_{4}$\n",
    "    * $\\frac{\\partial Z_{1}}{\\partial Z_{3}} = W_{3}$\n",
    "    * $\\frac{\\partial Z_{1}}{\\partial Z_{4}} = W_{4}$\n",
    "\n",
    "3. $Z_{2} = W_{5}*Z_{4}$\n",
    "    * $\\frac{\\partial Z_{2}}{\\partial Z_{4}} = W_{5}$\n",
    "\n",
    "4. $Z_{4} = W_{7}*X_{2}$\n",
    "    * $\\frac{\\partial Z_{4}}{\\partial X_{2}} = W_{7}$\n",
    "\n",
    "5. $Z_{3} = W_{6}*X_{1}$\n",
    "    * $\\frac{\\partial Z_{3}}{\\partial X_{1}} = W_{6}$\n",
    "\n",
    "$Loss =  (1/2)*(Y_{g} - \\hat{Y})^{2}$\n",
    "\n",
    "Note, that when we feed many points in a mini-batch at the same time, the mini-batch loss is the average of the loss calculated for each point.\n",
    "\n",
    "$Loss =  (1/2m)*(Y_{g} - \\hat{Y})^{2}$ where **m** is the number of points in the mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d19de5dcf2ec69901d459617001151e1",
     "grade": false,
     "grade_id": "cell-20e7cb49e4e99ddc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Images/Complex_Network.jpg\" width=500 height=500>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"Images/Complex_Network.jpg\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "485c492cffb6dcfca04cc6e714cd478a",
     "grade": false,
     "grade_id": "cell-eabd823e68ebe6d6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As explained below, we observe a pattern when calculating the $(\\frac{\\partial Loss}{\\partial Z_{i}})$ for each $Z_{i}$. If you are not interested in the math, then atleast take a look at the **Simplified Meaning** statements.\n",
    "We assume that **m** data points are sent in a mini-batch.\n",
    "\n",
    "1) $Error_{Pred} = \\frac{\\partial Loss}{\\partial \\hat{Y}} =  (1/m)*(\\hat{Y}-Y_{g})$\n",
    "\n",
    "\n",
    "2) $Error_{Z_{1}} = \\frac{\\partial Loss}{\\partial Z_{1}} =  \\frac{\\partial Loss}{\\partial \\hat{Y}}*\\frac{\\partial \\hat{Y}}{\\partial Z_{1}} = \\frac{\\partial Loss}{\\partial \\hat{Y}}*W_{1}$\n",
    "    \n",
    "----> **Simplified Meaning** : $Error_{Z_{1}} = Error_{Pred} * W_{1}$\n",
    "\n",
    "\n",
    "3) $Error_{Z_{2}} = \\frac{\\partial Loss}{\\partial Z_{2}} =  \\frac{\\partial Loss}{\\partial \\hat{Y}}*\\frac{\\partial \\hat{Y}}{\\partial Z_{2}} = \\frac{\\partial Loss}{\\partial \\hat{Y}}*W_{2}$\n",
    "\n",
    "----> **Simplified Meaning** : $Error_{Z_{2}} =  Error_{Pred} * W_{2}$\n",
    "\n",
    "\n",
    "4) $Loss_{Z_{3}} = \\frac{\\partial Loss}{\\partial Z_{3}} =  \\frac{\\partial Loss}{\\partial \\hat{Y}}*\\frac{\\partial \\hat{Y}}{\\partial Z_{3}} = \\frac{\\partial Loss}{\\partial \\hat{Y}}*( W_{1}*\\frac{\\partial Z_{1}}{\\partial Z_{3}} + W_{2}*\\frac{\\partial Z_{2}}{\\partial Z_{3}} ) =  (\\frac{\\partial Loss}{\\partial \\hat{Y}}*W_{1})*\\frac{\\partial Z_{1}}{\\partial Z_{3}} = (\\frac{\\partial Loss}{\\partial Z_{1}})*W_{3}$\n",
    "\n",
    "----> **Simplified Meaning** : $Error_{Z_{3}} =  Error_{Z_{1}} * W_{3}$\n",
    "\n",
    "\n",
    "5) $Loss_{Z_{4}} = \\frac{\\partial Loss}{\\partial Z_{4}} =  \\frac{\\partial Loss}{\\partial \\hat{Y}}*\\frac{\\partial \\hat{Y}}{\\partial Z_{4}} = \\frac{\\partial Loss}{\\partial \\hat{Y}}*( W_{1}*\\frac{\\partial Z_{1}}{\\partial Z_{4}} + W_{2}*\\frac{\\partial Z_{2}}{\\partial Z_{4}} ) =  (\\frac{\\partial Loss}{\\partial \\hat{Y}}*W_{1})*\\frac{\\partial Z_{1}}{\\partial Z_{4}} + (\\frac{\\partial Loss}{\\partial \\hat{Y}}*W_{2})*\\frac{\\partial Z_{2}}{\\partial Z_{4}} =  (\\frac{\\partial Loss}{\\partial Z_{1}})*W_{4} + (\\frac{\\partial Loss}{\\partial Z_{2}})*W_{5}$\n",
    "\n",
    "----> **Simplified Meaning** : $Error_{Z_{4}} =  Error_{Z_{1}} * W_{4} + Error_{Z_{2}} * W_{5}$\n",
    "\n",
    "\n",
    "A pictorial representation of these errors is shown below. This graph representation of errors is known as a **Error Graph**. If you observe this carefully, this is the same network but with the edges reversed. Hence, it is said, that the **flow of errors is exactly opposite to the flow of computation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ac32bab224a156f779a7c1c7f7ececc",
     "grade": false,
     "grade_id": "cell-0c98800ada6a467e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Images/Error_Graph.jpg\" width=500 height=500>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"Images/Error_Graph.jpg\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da159dd308e1bd22c52fe0bf140268d3",
     "grade": false,
     "grade_id": "cell-9d0bbea1e7e24a80",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Concept 2 - Connected neurons are like family\n",
    "\n",
    "If you have understood the error graph, it will be easy for you to grasp concept 2. Concept 2 states that in order to find the error of a neuron, we only require to know the errors of the neurons it is connected to. This can be verified by all the error equations we had calculated.\n",
    "\n",
    "*Connected neurons are like family*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e9aaf795c3e29b04a60e52836ef3601",
     "grade": false,
     "grade_id": "cell-aaa704051e302c00",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_Errors_of_Complex_Network(y_hat:np.ndarray,\n",
    "                                  y_ground:np.ndarray,\n",
    "                                  All_weights:OrderedDict,\n",
    "                                  All_Errors:OrderedDict) -> None:\n",
    "    \"\"\" Calculates the errors for the Complex Network and stores these errors in \"All_Errors\". \n",
    "    \n",
    "    Max Marks : 2\n",
    "    \n",
    "    Arguments:\n",
    "        \n",
    "        y_hat : Prediciton Array with size = [batch_size]\n",
    "        y_ground : Ground truth Array with size = [batch_size]\n",
    "        All_weights : Dictionary with all stored weights. The keys and values are listed below.\n",
    "                      All_weights[\"W1\"] contains weight W1\n",
    "                      All_weights[\"W2\"] contains weight W2\n",
    "                      All_weights[\"W3\"] contains weight W3\n",
    "                      All_weights[\"W4\"] contains weight W4\n",
    "                      All_weights[\"W5\"] contains weight W5\n",
    "                      All_weights[\"W6\"] contains weight W6\n",
    "                      All_weights[\"W7\"] contains weight W7\n",
    "        All_Errors : Dictionary with all stored errors. The keys and values are listed below.\n",
    "                     All_Errors[\"Pred\"] should contain the Error of Pred\n",
    "                     All_Errors[\"Z1\"] should contain the Error of Z1\n",
    "                     All_Errors[\"Z2\"] should contain the Error of Z2\n",
    "                     All_Errors[\"Z3\"] should contain the Error of Z3\n",
    "                     All_Errors[\"Z4\"] should contain the Error of Z4\n",
    "    \"\"\"\n",
    "    batch_size = len(y_ground)\n",
    "    \n",
    "    Error_Pred = (y_hat - y_ground)/batch_size\n",
    "    # Please calculate Error_Z1,Error_Z2,Error_Z3,Error_Z4\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    Error_Z1 = Error_Pred * All_weights[\"W1\"]\n",
    "    Error_Z2 = Error_Pred * All_weights[\"W2\"]\n",
    "    Error_Z3 = Error_Z1 * All_weights[\"W3\"]\n",
    "    Error_Z4 = Error_Z1 * All_weights[\"W4\"] + Error_Z2 * All_weights[\"W5\"]\n",
    "\n",
    "    try:\n",
    "        assert(Error_Pred.shape == (batch_size,))\n",
    "        assert(Error_Z1.shape == (batch_size,))\n",
    "        assert(Error_Z2.shape == (batch_size,))\n",
    "        assert(Error_Z3.shape == (batch_size,))\n",
    "        assert(Error_Z4.shape == (batch_size,))\n",
    "    except :\n",
    "        print(\"Error Pred : {}\".format(Error_Pred.shape))\n",
    "        print(\"Error Z1 : {}\".format(Error_Z1.shape))\n",
    "        print(\"Error Z2 : {}\".format(Error_Z2.shape))\n",
    "        print(\"Error Z3 : {}\".format(Error_Z3.shape))\n",
    "        print(\"Error Z4 : {}\".format(Error_Z4.shape))\n",
    "        raise ValueError(\"Something's wrong!\")\n",
    "    \n",
    "    All_Errors[\"Pred\"] = Error_Pred\n",
    "    All_Errors[\"Z1\"] = Error_Z1\n",
    "    All_Errors[\"Z2\"] = Error_Z2\n",
    "    All_Errors[\"Z3\"] = Error_Z3\n",
    "    All_Errors[\"Z4\"] = Error_Z4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9f88d9ac2ba4d72aa19a926479178aa",
     "grade": true,
     "grade_id": "cell-6f0a70a1b9c2a4be",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e39c6ac79c2ad6c0d5cc44b549f1db5",
     "grade": false,
     "grade_id": "cell-2e8e9bae9ad63c15",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Network Collapse & Adding Activation Functions\n",
    "\n",
    "When we do not add an activation function, we can perform a procedure known as **Network Collapse**. Consider, the *complex network* shown in the previous section. If we observe carefully, $\\hat{Y}$ can directly be written as a function of $X_{1}$ and $X_{2}$.\n",
    "\n",
    "For example, $\\hat{Y} = {W_{1}}_{New}*X_{1} + {W_{2}}_{New}*X_{2}$ where ${W_{1}}_{New}$ is expressed as $W_{1}*W_{3}*W_{6}.$ Can you find out ${W_{2}}_{New}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d962efd0b60c81824fa264dac6db291",
     "grade": false,
     "grade_id": "cell-9efb3cde02287e18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_W1_New(All_weights:OrderedDict)->float:\n",
    "    \"\"\" Calculates W1_New of the collapsed network.\n",
    "    \n",
    "    Arguments :\n",
    "        All_weights : Dictionary with all stored weights. The keys and values are listed below\n",
    "                          All_weights[\"W1\"] contains weight W1\n",
    "                          All_weights[\"W2\"] contains weight W2\n",
    "                          All_weights[\"W3\"] contains weight W3\n",
    "                          All_weights[\"W4\"] contains weight W4\n",
    "                          All_weights[\"W5\"] contains weight W5\n",
    "                          All_weights[\"W6\"] contains weight W6\n",
    "                          All_weights[\"W7\"] contains weight W7\n",
    "    \"\"\"\n",
    "    return All_weights[\"W1\"]*All_weights[\"W3\"]*All_weights[\"W6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cebe7d9449f56cdbb86457516bd44f96",
     "grade": false,
     "grade_id": "cell-438745c74e595807",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_W2_New(All_weights:OrderedDict)->float:\n",
    "    \"\"\" Calculates W2_New of the collapsed network.\n",
    "    \n",
    "    Max Marks : 1\n",
    "    \n",
    "    Arguments :\n",
    "        All_weights : Dictionary with all stored weights. The keys and values are listed below\n",
    "                          All_weights[\"W1\"] contains weight W1\n",
    "                          All_weights[\"W2\"] contains weight W2\n",
    "                          All_weights[\"W3\"] contains weight W3\n",
    "                          All_weights[\"W4\"] contains weight W4\n",
    "                          All_weights[\"W5\"] contains weight W5\n",
    "                          All_weights[\"W6\"] contains weight W6\n",
    "                          All_weights[\"W7\"] contains weight W7\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return All_weights[\"W2\"]*All_weights[\"W5\"]*All_weights[\"W7\"] + All_weights[\"W1\"]*All_weights[\"W4\"]*All_weights[\"W7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1853f048f96668b09b669191312d328",
     "grade": true,
     "grade_id": "cell-75f6efbd2e781c19",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!\n",
    "sample_weights = OrderedDict()\n",
    "sample_weights[\"W1\"] = 0.1\n",
    "sample_weights[\"W2\"] = 0.9\n",
    "sample_weights[\"W3\"] = 0.3\n",
    "sample_weights[\"W4\"] = 0.4\n",
    "sample_weights[\"W5\"] = 0.9\n",
    "sample_weights[\"W6\"] = 0.1\n",
    "sample_weights[\"W7\"] = 0.3\n",
    "\n",
    "assert( calculate_W2_New(sample_weights) == 0.255)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c1dedb7a6e2ab4a67a333e57035623e",
     "grade": false,
     "grade_id": "cell-841d677e6be4dc61",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We realize, that without adding an activation function, complex networks are indeed useless! Hence let us now add an *activation function*. Right below is an example of a very minimal network in order to showcase what differences arise in the backprop when we incorporate an activation function. In this example, we use the *sigmoid* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed2f9f5303c44a24ccea9cad08356c2c",
     "grade": false,
     "grade_id": "cell-168786529a486679",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Images/Activation.jpg\" width=500 height=500>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"Images/Activation.jpg\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a702e85286b60bbe0712bf6308ae7106",
     "grade": false,
     "grade_id": "cell-76dda9be24fc142d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let us list down all the computation equations.\n",
    "\n",
    "1. $\\hat{Y} = \\sigma{(Z_{2})}*W_{2}$\n",
    "    * $\\frac{\\partial \\hat{Y}}{\\partial Z_{2}} = W_{2}*\\frac{\\partial (\\sigma{(Z_{2})})}{\\partial Z_{2}} = W_{2}*\\sigma{(Z_{2})}*(1-\\sigma{(Z_{2})})$\n",
    "2. $Z_{2} = Z_{1}*W_{1}$\n",
    "    * $\\frac{\\partial Z_{2}}{\\partial Z_{1}} = W_{1}$\n",
    "    \n",
    "Let us list down all the backprop equations.\n",
    "\n",
    "1. $Error_{Pred} = \\frac{\\partial Loss}{\\partial \\hat{Y}} = (1/m)*(\\hat{Y} - Y_{g})$\n",
    "\n",
    "2. $Error_{Z_{2}} = \\frac{\\partial Loss}{\\partial Z_{2}}  = \\frac{\\partial Loss}{\\partial \\hat{Y}} * \\frac{\\partial \\hat{Y}}{\\partial Z_{2}} = Error_{Pred}*W_{2}*\\sigma{(Z_{2})}*(1-\\sigma{(Z_{2})}) = Error_{Pred}*W_{2}*differentation(\\sigma(Z_{2}))$\n",
    "\n",
    "3. $Error_{Z_{1}} = \\frac{\\partial Loss}{\\partial Z_{1}}  = (\\frac{\\partial Loss}{\\partial \\hat{Y}} * \\frac{\\partial \\hat{Y}}{\\partial Z_{2}}) * \\frac{\\partial Z_{2}}{\\partial Z_{1}} = (Error_{Z_{2}})*W_{1}$\n",
    "\n",
    "By observing equations 2 and 3 we understand a very simple thing. \n",
    "\n",
    "$Error_{Z_{2}}$ would have been $Error_{Pred}*W_{2}$ if there were no activation function. By adding an activation function, $Error_{Pred}*W_{2}$ was simply multiplied by the differentiation of the activation function which is $\\sigma{(Z_{2})}*(1-\\sigma{(Z_{2})})$.\n",
    "\n",
    "$Error_{Z_{1}}$ is the same as there is no activation function after $Z_{1}$.\n",
    "\n",
    "## Concept 3 - Multiply the Differentiation of the Activation function\n",
    "It is apparent now, that the only change in the backprop when adding an activation function, is the multiplication of the differentiation of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31718c04547581845a032e4f577d2f3f",
     "grade": false,
     "grade_id": "cell-b1e0dc60f5bb681e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x:np.ndarray) -> np.ndarray:\n",
    "    \"\"\" This function returns the output of sigmoid(x). Where sigmoid(x) = 1/(1+e^{-x}). Make sure that you\n",
    "    first make a copy of \"x\" using np.copy(). Perform operations on this copy and return it.\n",
    "    \n",
    "    Max Marks : 1\n",
    "    \n",
    "    Arguments:\n",
    "        x : Input to sigmoid function\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    X = np.copy(x);\n",
    "    X = 1 + np.exp(-X);\n",
    "    sigmoid = 1 / X;\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "227d16aac6e4700db8865f701c9ed38f",
     "grade": true,
     "grade_id": "cell-1dabd3a7967671b3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!\n",
    "\n",
    "assert(sigmoid(np.array([0])) == np.array([0.5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "639a277175b22ae16a7a5b2835bdd8ab",
     "grade": false,
     "grade_id": "cell-87be4c8eb3ebc1c4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def diff_sigmoid(x:np.ndarray) -> np.ndarray:\n",
    "    \"\"\" This function returns the differentiation of sigmoid(x). Where sigmoid(x) = 1/(1+e^{-x}). Make sure that you\n",
    "    first make a copy of \"x\" using np.copy(). Perform operations on this copy and return it.\n",
    "    \n",
    "    Max Marks : 1\n",
    "    \n",
    "    Arguments:\n",
    "        x : The input to sigmoid function\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    X = sigmoid(np.copy(x))\n",
    "    sigmoid_deri = X * (1 - X)\n",
    "    return sigmoid_deri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0fd5cee09d547bff4bdfc43c9fd5637",
     "grade": true,
     "grade_id": "cell-729c7850ea2091a7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!\n",
    "\n",
    "assert(diff_sigmoid(np.array([0])) == 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c88e4d2c5b262f577066694a757b413f",
     "grade": false,
     "grade_id": "cell-f20535d5633b32f5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_Errors_with_Activation_Fn(y_hat:np.ndarray,\n",
    "                                  y_ground:np.ndarray,\n",
    "                                  All_weights:OrderedDict,\n",
    "                                  All_Zs:OrderedDict,\n",
    "                                  All_Errors:OrderedDict) -> None:\n",
    "    \"\"\" Calculates the errors for the minimal network given above and stores these errors into \"All_Errors\".\n",
    "    To incorporate the differentiation of the activation function, you can use the \"diff_sigmoid\" function.\n",
    "    \n",
    "    Max Marks : 2\n",
    "    \n",
    "    Arguments:\n",
    "        \n",
    "        y_hat : Prediction Array with size = [batch_size]\n",
    "        y_ground : Ground truth Array with size = [batch_size]\n",
    "        All_weights : Dictionary with all stored weights. The keys and values are listed below\n",
    "                      All_weights[\"W1\"] contains weight W1\n",
    "                      All_weights[\"W2\"] contains weight W2\n",
    "        All_Zs : Dictionary with all stored outputs. The keys and values are listed below\n",
    "                 All_Zs[\"Z1\"] contains output Z1\n",
    "                 All_Zs[\"Z2\"] contains output Z2\n",
    "        All_Errors : Dictionary with all stored errors. The keys and values are listed below.\n",
    "                     All_Errors[\"Pred\"] should contain the Error of Pred\n",
    "                     All_Errors[\"Z1\"] should contain the Error of Z1\n",
    "                     All_Errors[\"Z2\"] should contain the Error of Z2\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(y_ground)\n",
    "    \n",
    "    Error_Pred = (y_hat - y_ground)/batch_size\n",
    "    # Please calculate Error_Z2 and Error_Z1\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    W1 = All_weights[\"W1\"]\n",
    "    W2 = All_weights[\"W2\"]\n",
    "    Z2 = All_Zs[\"Z2\"]\n",
    "    Error_Z2 = Error_Pred * W2 * Z2 * (1 - Z2)\n",
    "    Error_Z1 = Error_Z2 * W1\n",
    "\n",
    "    try:\n",
    "        assert(Error_Pred.shape == (batch_size,))\n",
    "        assert(Error_Z1.shape == (batch_size,))\n",
    "        assert(Error_Z2.shape == (batch_size,))\n",
    "    except :\n",
    "        print(\"Error Pred : {}\".format(Error_Pred.shape))\n",
    "        print(\"Error Z1 : {}\".format(Error_Z1.shape))\n",
    "        print(\"Error Z2 : {}\".format(Error_Z2.shape))\n",
    "        raise ValueError(\"Something's wrong!\")\n",
    "        \n",
    "    \n",
    "    All_Errors[\"Pred\"] = Error_Pred\n",
    "    All_Errors[\"Z2\"] = Error_Z2\n",
    "    All_Errors[\"Z1\"] = Error_Z1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6f80728cd13a16e4b31b03fd8317db8",
     "grade": true,
     "grade_id": "cell-1a33cd41b87a429d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b3f74764fb77ecd16a55a31dacdc1ff",
     "grade": false,
     "grade_id": "cell-e272cff481708a58",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let us now add an activation function to each node in the *complex network*. From now onwards, the combination of $Act_{i}(Z_{i})$ and $Z_{i}$ will be fused into a node called $N_{i}$. This modified network will be referred to as *Modified Network* in further discussions. \n",
    "\n",
    "The list of activation functions ($Act_{i}$) corresponding to each node ($N_{i}$) are mentioned below.\n",
    "\n",
    "$N_{1}$ => $Act_{1}$ => $ReLU$\n",
    "\n",
    "$N_{2}$ => $Act_{2}$ => $Sigmoid$\n",
    "\n",
    "$N_{3}$ => $Act_{3}$ => $ReLU$\n",
    "\n",
    "$N_{4}$ => $Act_{4}$ => $Sigmoid$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ae334ed48315d590a456c5647f0a82f",
     "grade": false,
     "grade_id": "cell-4e2c0eadd75a892c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Images/Modified_Network.jpg\" width=500 height=500>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"Images/Modified_Network.jpg\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "789950264d22898dee765dafc3f7888b",
     "grade": false,
     "grade_id": "cell-5cba5c83c7724c14",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(x:np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Returns the output of Rectified Linear Unit Function. ReLU(x) is max(0,x). Make sure that you\n",
    "    first make a copy of \"x\" using np.copy(). Perform operations on this copy and return it.\n",
    "    \n",
    "    Max Marks : 1\n",
    "    \n",
    "    Arguments:\n",
    "        x : Input to ReLU function\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    X = np.copy(x)\n",
    "    check = X < 0\n",
    "    X[check] = 0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a6627ab30a789489eff38f8d4eb681d",
     "grade": true,
     "grade_id": "cell-fc30c4af229f3223",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!\n",
    "\n",
    "assert( np.array_equal(ReLU(np.array([-0.2,0.5])),np.array([0,0.5])) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3d147ea3ef0688f812d3e281e27f819",
     "grade": false,
     "grade_id": "cell-f322550bbb512405",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def diff_ReLU(x:np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Returns the differentiation of ReLU(x). Note, make sure that the differentiation of ReLU(x) at x=0 is 0. Make sure that you\n",
    "    first make a copy of \"x\" using np.copy(). Perform operations on this copy and return it.\n",
    "    \n",
    "    Max Marks : 1\n",
    "    \n",
    "    Arguments:\n",
    "        x : Input to the ReLU function\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    X = np.copy(x)\n",
    "    X[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2145a8fc436741b2a557fc16bfb85fe1",
     "grade": true,
     "grade_id": "cell-e60c0549ce0f73f1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!\n",
    "\n",
    "assert( np.array_equal(diff_ReLU(np.array([0.9,-0.5,0.])), np.array([1.,0.,0.])) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e46ca68d9c96b89f35adfc5e5ff62e3",
     "grade": false,
     "grade_id": "cell-c6351f1e2fdd57b0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_Errors_of_Modified_Network(y_ground:np.ndarray,\n",
    "                                   All_weights:OrderedDict,\n",
    "                                   All_Zs:OrderedDict,\n",
    "                                   All_Errors:OrderedDict) -> None:\n",
    "    \"\"\" Calculates the errors of the modified network and stores these errors in \"All_Errors\". To incorporate the\n",
    "    differentiation of sigmoid and ReLU, you can use \"diff_sigmoid\" and \"diff_ReLU\".\n",
    "    \n",
    "    Max Marks : 4\n",
    "    \n",
    "    Arguments :\n",
    "        y_ground : Ground truth Array with size = [batch_size]\n",
    "        \n",
    "        All_weights : Dictionary with all stored weights. The keys and values are listed below\n",
    "                          All_weights[\"W1\"] contains weight W1\n",
    "                          All_weights[\"W2\"] contains weight W2\n",
    "                          All_weights[\"W3\"] contains weight W3\n",
    "                          All_weights[\"W4\"] contains weight W4\n",
    "                          All_weights[\"W5\"] contains weight W5\n",
    "                          All_weights[\"W6\"] contains weight W6\n",
    "                          All_weights[\"W7\"] contains weight W7\n",
    "        \n",
    "        All_Zs : Dictionary with all stored outputs. The keys and values are listed below\n",
    "                    All_Zs[\"Pred\"] contains output Pred\n",
    "                    All_Zs[\"Z1\"] contains output Z1\n",
    "                    All_Zs[\"Z2\"] contains output Z2\n",
    "                    All_Zs[\"Z3\"] contains output Z3\n",
    "                    All_Zs[\"Z4\"] contains output Z4\n",
    "\n",
    "        All_Errors : Dictionary with all stored errors. The keys and values are listed below.\n",
    "                         All_Errors[\"Pred\"] should contain the Error of Pred\n",
    "                         All_Errors[\"Z1\"] should contain the Error of Z1\n",
    "                         All_Errors[\"Z2\"] should contain the Error of Z2\n",
    "                         All_Errors[\"Z3\"] should contain the Error of Z3\n",
    "                         All_Errors[\"Z4\"] should contain the Error of Z4\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(y_ground)    \n",
    "    Error_Pred = (All_Zs[\"Pred\"] - y_ground)/batch_size\n",
    "    # Please calculate Error_Z1, Error_Z2, Error_Z3, Error_Z4\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    try:\n",
    "        assert(Error_Pred.shape == (batch_size,))\n",
    "        assert(Error_Z1.shape == (batch_size,))\n",
    "        assert(Error_Z2.shape == (batch_size,))\n",
    "        assert(Error_Z3.shape == (batch_size,))\n",
    "        assert(Error_Z4.shape == (batch_size,))\n",
    "    except :\n",
    "        print(\"Error Pred : {}\".format(Error_Pred.shape))\n",
    "        print(\"Error Z1 : {}\".format(Error_Z1.shape))\n",
    "        print(\"Error Z2 : {}\".format(Error_Z2.shape))\n",
    "        print(\"Error Z3 : {}\".format(Error_Z3.shape))\n",
    "        print(\"Error Z4 : {}\".format(Error_Z4.shape))\n",
    "        raise ValueError(\"Something's wrong!\")\n",
    "    \n",
    "    \n",
    "    All_Errors[\"Pred\"] = Error_Pred\n",
    "    All_Errors[\"Z1\"] = Error_Z1\n",
    "    All_Errors[\"Z2\"] = Error_Z2\n",
    "    All_Errors[\"Z3\"] = Error_Z3\n",
    "    All_Errors[\"Z4\"] = Error_Z4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20940a0ab5546522c9ebf6c37590695c",
     "grade": true,
     "grade_id": "cell-db1aaf9f52d4ce42",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af58656d3289dca6a6138d2710cc7553",
     "grade": false,
     "grade_id": "cell-b55e462057ce61f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Updating the weight\n",
    "\n",
    "Whenever we train neural networks, we perform two steps. The first step is to perform backpropagation and get the gradients. The second step is to use these gradients to change the weights. This second step is also known as **Optimization**.\n",
    "\n",
    "If you recall, the update equation mentioned at the beginning was $W_{i} = W_{i} - \\alpha*\\frac{\\partial Loss}{\\partial W_{i}}$. But up until now, we have been finding out gradients for **nodes** i.e. $\\frac{\\partial Loss}{\\partial Z_{i}}$ and not for **weights** i.e. $\\frac{\\partial Loss}{\\partial W_{i}}$.\n",
    "\n",
    "\n",
    "The procedure to calculate $\\frac{\\partial Loss}{\\partial W_{i}}$ is very straight-forward. Consider, that the input to the weight $W_{i}$ is $Input_{i}$ and that the other end of the weight is connected to $Output_{i}$.\n",
    "\n",
    "Then $\\frac{\\partial Loss}{\\partial W_{i}} = \\frac{\\partial Loss}{\\partial Output_{i}}*Input_{i}$.\n",
    "\n",
    "Let us take two examples to better understand this.\n",
    "\n",
    "1. Consider $W_{7}$ in the *modified network*, the input is $X_{2}$ and the other end is connected to $Z_{4}$.\n",
    "    * Hence $\\frac{\\partial Loss}{\\partial W_{7}} = \\frac{\\partial Loss}{\\partial Z_{4}}*X_{2} = Error_{Z_{4}}*X_{2} $\n",
    "2. Consider $W_{5}$ in the *modified network*, the input is $N_{4}$ and the other end is connected to $Z_{2}$. If you don't understand why the input is in $N$ terms and output is in $Z$ terms, break each node $N$ into $Z$ and $Act(Z)$ and then see the connections.\n",
    "    * Hence $\\frac{\\partial Loss}{\\partial W_{5}} = \\frac{\\partial Loss}{\\partial Z_{2}}*N_{4} = Error_{Z_{2}}*N_{4}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d8df3f4e05eef02afab7bfe9e1186de",
     "grade": false,
     "grade_id": "cell-cd581d8312acd81b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_Gradient_Change_to_Modified_Network(alpha:float,\n",
    "                                               All_weights:OrderedDict,\n",
    "                                               All_Errors:OrderedDict,\n",
    "                                               All_Nodes:OrderedDict,\n",
    "                                               All_Xs:OrderedDict) -> None:\n",
    "    \"\"\" Apply Gradient Change to weights in the modified network. And store the new weights back in the \n",
    "    'All_weights' dictionary. \n",
    "    \n",
    "    Max Marks : 3\n",
    "    \n",
    "    Arguments:\n",
    "        \n",
    "        alpha : Learning rate for gradient descent\n",
    "        \n",
    "        All_weights : Dictionary with all stored weights. The keys and values are listed below\n",
    "                          All_weights[\"W1\"] contains weight W1\n",
    "                          All_weights[\"W2\"] contains weight W2\n",
    "                          All_weights[\"W3\"] contains weight W3\n",
    "                          All_weights[\"W4\"] contains weight W4\n",
    "                          All_weights[\"W5\"] contains weight W5\n",
    "                          All_weights[\"W6\"] contains weight W6\n",
    "                          All_weights[\"W7\"] contains weight W7\n",
    "                          \n",
    "        All_Errors : Dictionary with all stored errors. The keys and values are listed below.\n",
    "                     All_Errors[\"Pred\"] contains the Error of Pred\n",
    "                     All_Errors[\"Z1\"] contains the Error of Z1\n",
    "                     All_Errors[\"Z2\"] contains the Error of Z2\n",
    "                     All_Errors[\"Z3\"] contains the Error of Z3\n",
    "                     All_Errors[\"Z4\"] contains the Error of Z4\n",
    "                     \n",
    "        All_Nodes : Dictionary with all stored outputs. The keys and values are listed below\n",
    "                    All_Nodes[\"Pred\"] contains output Pred\n",
    "                    All_Nodes[\"N1\"] contains output N1\n",
    "                    All_Nodes[\"N2\"] contains output N2\n",
    "                    All_Nodes[\"N3\"] contains output N3\n",
    "                    All_Nodes[\"N4\"] contains output N4\n",
    "        \n",
    "        All_Xs : Dictionary with all stored inputs. The keys and values are listed below\n",
    "                    All_Nodes[\"X1\"] contains input X1\n",
    "                    All_Nodes[\"X2\"] contains input X2\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = len(All_Xs[\"X1\"])\n",
    "    \n",
    "    # Calculate the gradients\n",
    "    W1_gradient = All_Errors[\"Pred\"]*All_Nodes[\"N1\"]\n",
    "    # Please calculate W2_gradient, W3_gradient, .... ,W7_gradient\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "     # Making sure everything is correct\n",
    "    try :\n",
    "        assert(W1_gradient.shape == (batch_size,))\n",
    "        assert(W2_gradient.shape == (batch_size,))\n",
    "        assert(W3_gradient.shape == (batch_size,))\n",
    "        assert(W4_gradient.shape == (batch_size,))\n",
    "        assert(W5_gradient.shape == (batch_size,))\n",
    "        assert(W6_gradient.shape == (batch_size,))\n",
    "        assert(W7_gradient.shape == (batch_size,))\n",
    "    except :\n",
    "        print(\"W1 Gradient : {}\".format(W1_gradient.shape))\n",
    "        print(\"W2 Gradient : {}\".format(W2_gradient.shape))\n",
    "        print(\"W3 Gradient : {}\".format(W3_gradient.shape))\n",
    "        print(\"W4 Gradient : {}\".format(W4_gradient.shape))\n",
    "        print(\"W5 Gradient : {}\".format(W5_gradient.shape))\n",
    "        print(\"W6 Gradient : {}\".format(W6_gradient.shape))\n",
    "        print(\"W7 Gradient : {}\".format(W7_gradient.shape))\n",
    "    \n",
    "    All_weights[\"W1\"] -= alpha*np.sum(W1_gradient)\n",
    "    # Please update all the weights W2,W3 .... W7\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "212a1ccc78801e2910427b86caa67b81",
     "grade": true,
     "grade_id": "cell-e82e3097b9190189",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "031aa92991c94d7bbbdb8ddc7a6bc457",
     "grade": false,
     "grade_id": "cell-416108e14a69b422",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "We now know how to calculate the backprop equations and how to use these gradients to change the weights. Let us assemble all these components together to complete the backprop and optimization of the *modified network*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e1097c3aa7bfb4a9e2979e16aee7914",
     "grade": false,
     "grade_id": "cell-0f211432b430b74d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_Weights():\n",
    "    # Initialize the Weights of the network\n",
    "    All_weights = OrderedDict()\n",
    "    All_weights[\"W1\"] = np.abs(np.random.randn(1)).item()\n",
    "    All_weights[\"W2\"] = np.abs(np.random.randn(1)).item()\n",
    "    All_weights[\"W3\"] = np.abs(np.random.randn(1)).item()\n",
    "    All_weights[\"W4\"] = np.abs(np.random.randn(1)).item()\n",
    "    All_weights[\"W5\"] = np.abs(np.random.randn(1)).item()\n",
    "    All_weights[\"W6\"] = np.abs(np.random.randn(1)).item()\n",
    "    All_weights[\"W7\"] = np.abs(np.random.randn(1)).item()\n",
    "    \n",
    "    return All_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fcd5c0291021fd8b27db63bf2508174",
     "grade": false,
     "grade_id": "cell-33e67e7ac740ee34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_Errors():\n",
    "    # Initialize the Errors of the network\n",
    "    All_Errors = OrderedDict()\n",
    "    All_Errors[\"Pred\"] = None\n",
    "    All_Errors[\"Z1\"] = None\n",
    "    All_Errors[\"Z2\"] = None\n",
    "    All_Errors[\"Z3\"] = None\n",
    "    All_Errors[\"Z4\"] = None\n",
    "    \n",
    "    return All_Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff1e97c3600ff640f0bb50020e7d7d3f",
     "grade": false,
     "grade_id": "cell-17646832b00a4566",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_Nodes():\n",
    "    # Initialize the Nodes of the network\n",
    "    All_Nodes = OrderedDict()\n",
    "    All_Nodes[\"Pred\"] = None\n",
    "    All_Nodes[\"N1\"] = None\n",
    "    All_Nodes[\"N2\"] = None\n",
    "    All_Nodes[\"N3\"] = None\n",
    "    All_Nodes[\"N4\"] = None\n",
    "    \n",
    "    return All_Nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cf44839f4035fc9f0965d628dbf3035",
     "grade": false,
     "grade_id": "cell-fee926fb614e33a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_Zs():\n",
    "    # Initialize the Z's of the network\n",
    "    All_Zs = OrderedDict()\n",
    "    All_Zs[\"Pred\"] = None\n",
    "    All_Zs[\"Z1\"] = None\n",
    "    All_Zs[\"Z2\"] = None\n",
    "    All_Zs[\"Z3\"] = None\n",
    "    All_Zs[\"Z4\"] = None\n",
    "    \n",
    "    return All_Zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b59f7856ae27c54d926d674a63e9eaa",
     "grade": false,
     "grade_id": "cell-0c33a0910914f684",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_data(x1:np.ndarray,\n",
    "                 x2:np.ndarray,\n",
    "                 y:np.ndarray):\n",
    "    \"\"\" Shuffles data after every epoch \"\"\"\n",
    "    num_points = len(x1)\n",
    "    g = np.arange(num_points)\n",
    "    np.random.shuffle(g)\n",
    "    \n",
    "    x1 = x1[g]\n",
    "    x2 = x2[g]\n",
    "    y = y[g]\n",
    "    \n",
    "    return x1,x2,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09ba239a85a540ea859ef4a009f89f8f",
     "grade": false,
     "grade_id": "cell-ba34b8eb532d6257",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_prop(x1:np.ndarray,\n",
    "                x2:np.ndarray,\n",
    "                All_weights:OrderedDict,\n",
    "                All_Nodes:OrderedDict,\n",
    "                All_Zs:OrderedDict) -> None :\n",
    "    \"\"\" Perform forward prop of modified network and store all the node values in the All_Nodes and the \n",
    "    All_Zs dictionaries. Use the \"sigmoid\" and \"ReLU\" functions you implemented earlier.\n",
    "    \n",
    "    Max Marks : 2\n",
    "    \n",
    "    Arguments :\n",
    "        \n",
    "        x1 : The x1 data points of the mini-batch\n",
    "        \n",
    "        x2 : The x2 data points of the mini-batch\n",
    "        \n",
    "        All_weights : Dictionary with all stored weights. The keys and values are listed below\n",
    "                          All_weights[\"W1\"] contains weight W1\n",
    "                          All_weights[\"W2\"] contains weight W2\n",
    "                          All_weights[\"W3\"] contains weight W3\n",
    "                          All_weights[\"W4\"] contains weight W4\n",
    "                          All_weights[\"W5\"] contains weight W5\n",
    "                          All_weights[\"W6\"] contains weight W6\n",
    "                          All_weights[\"W7\"] contains weight W7\n",
    "        \n",
    "        All_Nodes : Dictionary with all stored outputs. The keys and values are listed below\n",
    "                    All_Nodes[\"Pred\"] contains output Pred\n",
    "                    All_Nodes[\"N1\"] contains output N1\n",
    "                    All_Nodes[\"N2\"] contains output N2\n",
    "                    All_Nodes[\"N3\"] contains output N3\n",
    "                    All_Nodes[\"N4\"] contains output N4\n",
    "                    \n",
    "        All_Zs : Dictionary with all stored outputs. The keys and values are listed below\n",
    "                    All_Zs[\"Pred\"] contains output Pred\n",
    "                    All_Zs[\"Z1\"] contains output Z1\n",
    "                    All_Zs[\"Z2\"] contains output Z2\n",
    "                    All_Zs[\"Z3\"] contains output Z3\n",
    "                    All_Zs[\"Z4\"] contains output Z4\n",
    "    \"\"\"\n",
    "    \n",
    "    All_Zs[\"Z3\"] = All_weights[\"W6\"]*x1\n",
    "    All_Nodes[\"N3\"] = ReLU(All_Zs[\"Z3\"])\n",
    "    \n",
    "    # Please calculate Z2,N2,Z3,N3,Z4,N4,Pred \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    assert(np.array_equal(All_Zs[\"Pred\"],All_Nodes[\"Pred\"]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "386da3a1c3472fe912346c495446417c",
     "grade": true,
     "grade_id": "cell-8f0d9d338ca321a7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b53794c8c61bd9248673d991c0ffa47d",
     "grade": false,
     "grade_id": "cell-42b5d651e0c8b61e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def main_function(epoch_num:int,batch_size:int, data_points:int, alpha:float,\n",
    "                  x1_data:np.ndarray, x2_data:np.ndarray, y_data:np.ndarray,\n",
    "                  All_Weights:OrderedDict, All_Nodes:OrderedDict, All_Zs:OrderedDict,\n",
    "                  All_Errors:OrderedDict) -> None:\n",
    "    \"\"\" The main function that will train your neural network for 1 epoch.\n",
    "    \n",
    "    Max Marks : 1\n",
    "    \n",
    "    Arguments:\n",
    "        epoch_num : Count of which epoch is running\n",
    "        batch_size : The batch size of each mini-batch input\n",
    "        data_points : The total number of training data points\n",
    "        alpha : The learning rate of training\n",
    "        x1_data : All the x1 data points\n",
    "        x2_data : All the x2 data points\n",
    "        y_data : All the groundtruths\n",
    "        All_Weights : A dictionary containing all the weights of the network\n",
    "        All_Nodes : A dictionary containing all the outputs after activations of the network\n",
    "                    Ex.  (N1, N2, N3 and N4) \n",
    "        All_Zs : A dictionary containing all the outputs before activations of the network\n",
    "                 Ex. (Z1, Z2, Z3, Z4)\n",
    "        All_Errors : A dictionary containing all the errors of the network\n",
    "                     Ex. Error_Pred, Error_Z1, Error_Z2 and so on...\n",
    "    \"\"\"\n",
    "      \n",
    "    num_batches = int(np.ceil(data_points/batch_size))\n",
    "    total_loss = 0\n",
    "\n",
    "    # For each mini-batch\n",
    "    for k in range(0,num_batches):\n",
    "\n",
    "        # Input data\n",
    "        x1_input = np.copy(np.squeeze(x1_data[k*batch_size:min((k+1)*batch_size,data_points)]))\n",
    "        x2_input = np.copy(np.squeeze(x2_data[k*batch_size:min((k+1)*batch_size,data_points)]))\n",
    "        y_input = np.copy(np.squeeze(y_data[k*batch_size:min((k+1)*batch_size,data_points)]))\n",
    "\n",
    "        # Perform forward propagation - Call your appropriate function\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Perform back propagation to calculate the partial derivatives - Call your appropriate function\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Perform optimization and change weights - Call your appropriate function\n",
    "        All_Xs = OrderedDict()\n",
    "        All_Xs[\"X1\"] = x1_input\n",
    "        All_Xs[\"X2\"] = x2_input\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Update running loss\n",
    "        total_loss += np.sum((y_input-All_Nodes[\"Pred\"])**2)\n",
    "\n",
    "    print(\"Student : Epoch {} : Loss : {:.2f}\".format(epoch_num,total_loss/data_points))\n",
    "    print(\"\\tW1 : {:.2f}, W2 : {:.2f}, W3 : {:.2f}, W4 : {:.2f}, W5 : {:.2f}, W6 : {:.2f}, W7 : {:.2f}\\n\".format(All_Weights[\"W1\"],All_Weights[\"W2\"],All_Weights[\"W3\"],All_Weights[\"W4\"],All_Weights[\"W5\"],All_Weights[\"W6\"],All_Weights[\"W7\"]))    \n",
    "\n",
    "\n",
    "def check_more() :\n",
    "    num_epochs = 1000\n",
    "    batch_size = 10\n",
    "    data_points = 10000\n",
    "    alpha = 0.001\n",
    "\n",
    "    x1 = np.random.randn(data_points,1)\n",
    "    x2 = np.random.randn(data_points,1)\n",
    "    y = 3*ReLU(x1) + 4*x2\n",
    "    All_Weights = init_Weights()\n",
    "    All_Nodes = init_Nodes()\n",
    "    All_Zs = init_Zs()\n",
    "    All_Errors = init_Errors()\n",
    "\n",
    "\n",
    "    print(\"Initial W1 : {:.2f}, W2 : {:.2f}, W3 : {:.2f}, W4 : {:.2f}, W5 : {:.2f}, W6 : {:.2f}, W7 : {:.2f}\".format(All_Weights[\"W1\"],All_Weights[\"W2\"],All_Weights[\"W3\"],All_Weights[\"W4\"],All_Weights[\"W5\"],All_Weights[\"W6\"],All_Weights[\"W7\"]))\n",
    "    for k in range(num_epochs) :\n",
    "        x1,x2,y = shuffle_data(x1,x2,y)\n",
    "        main_function(k,batch_size,data_points,alpha,x1,x2,y,All_Weights,All_Nodes,All_Zs,All_Errors)\n",
    "\n",
    "check_more()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a8bbaae4cb8a354198a3720ce0e2f03",
     "grade": true,
     "grade_id": "cell-0854a6f9e587cb53",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please run these hidden tests !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
